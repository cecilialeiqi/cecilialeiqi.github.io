<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Qi Lei (雷琦)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html" class="current">Main</a></div>
<div class="menu-item"><a href="bio.html">Bio&nbsp;and&nbsp;CV</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="papers.html">Publication&nbsp;List</a></div>
<div class="menu-item"><a href="https://github.com/cecilialeiqi/">Github</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=kGOgaowAAAAJ&hl=en">Google&nbsp;Scholar</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Qi Lei (雷琦)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="Qi Lei.jpg"><img src="Qi Lei.jpg" alt="Qi Lei UT Austin" width="110px" height="150px" /></a>&nbsp;</td>
<td align="left"><p>Email: qilei at princeton.edu</p>
<p>Website: <a href="cecilialeiqi.github.io">http://cecilialeiqi.github.io</a></p>
</td></tr></table>
<p>I am an associate research scholar at ECE, Princeton University. I am fortunate to have Prof. <a href="http://jasondlee88.github.io">Jason Lee</a> as my research mentor. I received my Ph.D. from <a href="https://www.oden.utexas.edu/">Oden Institute for Computational Engineering and Sciences</a>, the
University of Texas at Austin, advised by <a href="http://users.ece.utexas.edu/~dimakis/index.html">Alexandros G. Dimakis</a> and <a href="http://www.cs.utexas.edu/~inderjit/">Inderjit S. Dhillon</a>. 
I was also a member of the <a href="http://bigdata.ices.utexas.edu/">Center for Big Data Analytics</a> and <a href="https://wncg.org/">Wireless Networking &amp; Communications Group</a>. I visited <a href="https://www.ias.edu/">IAS</a> for the <a href="http://www.math.ias.edu/theoretical_machine_learning">Theoretical Machine Learning program</a> from September 2019 to July 2020. Prior to that, I was a Research Fellow at the Simons Institute for the Theory of Computing at UC Berkeley for the program on Foundations of Deep Learning.</p>
<p>Starting from Sept 2022, I will be an Assistant Professor of Mathematics and Data Science at the <a href="https://www.courant.nyu.edu/">Courant Institute of Mathematical Sciences</a> and the <a href="https://cds.nyu.edu/">Center for Data Science</a> at <a href="https://www.nyu.edu/">NYU</a>.</p>
<p>My research interests are <b>machine learning</b>, <b>deep learning</b>, and <b>optimization</b>. Specifically, I am interested in developing sample- and computationally efficient algorithms for some fundamental machine learning problems. </p>
<p>(<a href="cv_2.pdf">Curriculum Vitae</a>, <a href="https://github.com/cecilialeiqi/">Github</a>, <a href="https://scholar.google.com/citations?user=kGOgaowAAAAJ&amp;hl=en">Google Scholar</a>)</p>
<h2>News and Announcement</h2>
<p><a href="04/2022">04/2022</a> Invited talk at <a href="https://math.dartmouth.edu/~acms/">Dartmouth ACMS</a> </p>
<p><a href="03/2022">03/2022</a> New papers out:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.12230">Sample Efficiency of Data Augmentation Consistency Regularization</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.15664">Nearly Minimax Algorithms for Linear Bandits with Shared Representation</a></p>
</li>
</ul>
<p><a href="02/2022">02/2022</a> Invited talk at <a href="https://simons.berkeley.edu/workshops/games2022-1">Adversarial Approaches in Machine Learning workshop</a> at Simons Institute</p>
<p><a href="12/2021">12/2021</a> Invited talk at the <a href="https://drive.google.com/file/d/1wAEyNqGaY_GrnoUXXdSXbLlNdGIBLSu-/view">USC Machine Learning Symposium</a> </p>
<p><a href="12/2021">12/2021</a> Invited talk in the <a href="https://www.ece.gatech.edu/calendar/day/2021/12/03/108208?utm_source=ECE+Lists+2019&amp;utm_campaign=a5a1732812-EMAIL_CAMPAIGN_2019_08_06_08_30_COPY_01&amp;utm_medium=email&amp;utm_term=0_0324b14402-a5a1732812-103287103">CSIP seminar at Gatech</a></p>
<p><a href="11/2021">11/2021</a> Invited talk in the <a href="https://ellis.ist.ac.at/">ELLIS talk series</a> on <a href="https://talks-calendar.app.ist.ac.at/events/3329">&lsquo;&lsquo;Provable representation learning"</a></p>
<p><a href="10/2021">10/2021</a> I'm honored to be selected as a <a href="https://ml.umd.edu/rising-stars">rising star in Machine Learning</a> at the University of Maryland</p>
<p><a href="10/2021">10/2021</a> I'm honored to be selected as a <a href="https://risingstars21-eecs.mit.edu/lei-qi/">rising star in EECS</a> at MIT</p>
<p><a href="10/2021">10/2021</a> I'm invited to give a talk on <a href="https://simons.berkeley.edu/talks/optimal-gradient-based-algorithms-non-concave-bandit-optimization">non-concave bandit optimization</a> at <a href="https://simons.berkeley.edu/workshops/schedule/16590">the Sampling Algorithms and Geometries on Probability Distributions workshop</a> at Simons Institute</p>
<p><a href="09/2021">09/2021</a> Four papers accepted at NeurIPS 2021:</p>
<ul>
<li><p>Baihe Huang*, Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, <b>Qi Lei</b>*, Runzhe Wang*, Jiaqi Yang* <a href="https://arxiv.org/abs/2107.04518">&lsquo;&lsquo;Optimal Gradient-based Algorithms for Non-concave Bandit Optimization"</a></p>
</li>
<li><p>Baihe Huang*, Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, <b>Qi Lei</b>*, Runzhe Wang*, Jiaqi Yang* <a href="https://arxiv.org/abs/2107.06466">&lsquo;&lsquo;Going Beyond Linear RL: Sample Efficient Neural Function Approximation</a> </p>
</li>
<li><p>Kurtland Chua, <b>Qi Lei</b>, Jason D. Lee, <a href="https://arxiv.org/abs/2105.02221">&lsquo;&lsquo;How fine-tuning allows for effective meta-learning"</a></p>
</li>
<li><p>Jason D Lee*, <b>Qi Lei</b>*, Nikunj Saunshi*, Jiacheng Zhuo*, <a href="https://arxiv.org/abs/2008.01064">&lsquo;&lsquo;Predicting What You Already Know Helps: Provable Self-Supervised Learning"</a></p>
</li>
</ul>
<p><a href="09/2021">09/2021</a> Invited talk at <a href="http://bliss.eecs.berkeley.edu/Seminar/fa21/qi.html">BLISS seminar</a></p>
<p><a href="07/2021">07/2021</a> New papers out:</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2107.04518">Optimal Gradient-based Algorithms for Non-concave Bandit Optimization</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.06466">Going Beyond Linear RL: Sample Efficient Neural Function Approximation</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2107.02377">A Short Note on the Relationship of Information Gain and Eluder Dimension</a></p>
</li>
</ul>
<p><a href="05/2021">05/2021</a> Three papers are accepted at ICML 2021:</p>
<ul>
<li><p><b>Qi Lei</b>, Wei Hu, Jason D. Lee. <a href="https://arxiv.org/abs/2106.12108">&lsquo;&lsquo;Near-Optimal Linear Regression under Distribution Shift"</a></p>
</li>
<li><p>Tianle Cai*, Ruiqi Gao*, Jason D Lee*, <b>Qi Lei</b>*.  <a href="https://arxiv.org/abs/2102.11203">&lsquo;&lsquo;A Theory of Label Propagation for Subpopulation Shift"</a></p>
</li>
<li><p>Jay Whang, <b>Qi Lei</b>, Alexandros G. Dimakis. <a href="https://arxiv.org/abs/2003.08089">&lsquo;&lsquo;Solving Inverse Problems with a Flow-based Noise Model"</a></p>
</li>
</ul>
<p><a href="05/2021">05/2021</a> I'm invited to give a talk on <a href="https://cms.caltech.edu/events/90169">Provable Representation Learning</a> at <a href="https://eas.caltech.edu/young_investigators">Caltech Young Investigators Lecture</a></p>
<p><a href="05/2021">05/2021</a> New paper out:
<a href="https://arxiv.org/abs/2105.02221">How Fine-Tuning Allows for Effective Meta-Learning</a></p>
<h2>Selected Papers</h2>
<p>(<a href="papers.html">full publication list</a>)</p>
<p>6. Baihe Huang*, Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, <b>Qi Lei</b>*, Runzhe Wang*, and Jiaqi Yang*.
<a href="https://arxiv.org/abs/2107.04518">&ldquo;Optimal Gradient-based Algorithms for Non-concave Bandit Optimization&rdquo;</a>, to appear at NeurIPS 2021</p>
<p>5. Jason D. Lee*, <b>Qi Lei</b>*, Nikunj Saunshi*, Jiacheng Zhuo*. <a href="https://arxiv.org/abs/2008.01064">&ldquo;Predicting What You Already Know Helps: Provable Self-Supervised Learning&rdquo;</a>, to appear at NeurIPS 2021</p>
<p>4. Simon S. Du*, Wei Hu*, Sham M. Kakade*, Jason D. Lee*, <b>Qi Lei</b>*.  <a href="https://arxiv.org/abs/2002.09434">&ldquo;Few-Shot Learning via Learning the Representation, Provably&rdquo;</a>, <i>The International Conference on Learning Representations (ICLR) 2021</i></p>
<p>3. <b>Qi Lei</b>, Jason D. Lee, Alexandros G. Dimakis, Constantinos Daskalakis. <a href="https://arxiv.org/abs/1910.07030">&ldquo;SGD Learns One-Layer Networks in WGANs&rdquo;</a>, <i>Proc. of International Conference of Machine Learning (ICML) 2020</i></p>
<p>2. <b>Qi Lei</b>*, Lingfei Wu*, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. Dhillon, Michael Witbrock. <a href="https://arxiv.org/abs/1812.00151">&ldquo;Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification&rdquo;</a>, <i>Systems and Machine Learning (sysML). 2019</i> <a href="https://github.com/cecilialeiqi/adversarial_text">(code</a>, <a href="discrete_attack.pdf">slides)</a> </p>
<ul>
<li><p>Press coverage: <a href="https://www.nature.com/articles/d41586-019-01510-1?utm_source=twt_nnc&amp;utm_medium=social&amp;utm_campaign=naturenews&amp;sf212595612=1">&lt;Nature Story&gt;</a> <a href="https://venturebeat.com/2019/04/01/text-based-ai-models-are-vulnerable-to-paraphrasing-attacks-researchers-find/">&lt;Vecturebeat&gt;</a> <a href="https://bdtechtalks.com/2019/04/02/ai-nlp-paraphrasing-adversarial-attacks/">&lt;Tech Talks&gt;</a> <a href="https://www.jiqizhixin.com/articles/2019-03-27-10?from=synced&amp;keyword=SysML%202019">&lt;机器之心&gt;</a></p>
</li>
</ul>
<p>1. Rashish Tandon, <b>Qi Lei</b>, Alexandros G. Dimakis, Nikos Karampatziakis, <a href="https://arxiv.org/abs/1612.03301">&ldquo;Gradient Coding: Avoiding Stragglers in Distributed Learning&rdquo;</a>, <i>Proc. of International Conference of Machine Learning (ICML), 2017</i> <a href="https://github.com/rashisht1/gradient_coding">(code)</a></p>
<div id="footer">
<div id="footer-text">
Page generated 2022-04-11 18:02:46 Eastern Daylight Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
