# jemdoc: menu{MENU}{index.html}
# The first line of this file is a special command that tells jemdoc which menu
# entry in the file named MENU to associate this page with.
= Qi Lei (雷琦)

~~~
{}{img_left}{Qi Lei.jpg}{Qi Lei NYU}{173px}{225px}{Qi Lei.jpg}


Assistant Professor of Mathematics and Data Science, and, by courtesy,\n
Assistant Professor of Computer Science,\n
Member of [https://wp.nyu.edu/cilvr/ CILVR lab],\n
Member of [https://mad.cds.nyu.edu/ Math and Data],\n
[https://www.courant.nyu.edu/ Courant Institute of Mathematical Sciences] and [https://cds.nyu.edu/ Center for Data Science],\n
[https://www.nyu.edu/ New York University]


Email: ql518 at nyu.edu
~~


== Research Overview

My research interests are *machine learning*, *deep learning*, and *optimization*. Specifically, I am interested in developing sample- and computationally efficient algorithms for some fundamental machine learning problems. 
 
([cv_2.pdf Curriculum Vitae], [https://github.com/cecilialeiqi/ Github], [https://scholar.google.com/citations?user=kGOgaowAAAAJ&hl=en Google Scholar])

== Advertisement

I am actively looking for self-motivated and proactive students to work with. You are welcome to shoot me an email with your CV and short research plans/interests. (You may refer to this [https://docs.google.com/document/d/1D1qBdVsTXb4VF18uAO2OfMRq9NNGTtkb3Lx2UYE3HS4/edit?usp=sharing link] to see whether our research interests match.) 

For Ph.D. applicants, please apply to [https://math.nyu.edu/dynamic/graduate/phd-mathematics/admission-policies/ Courant Mathematics] or [https://cds.nyu.edu/phd-program/ Center for Data Science] whichever you see fit and mention my name in your application. I do not plan to admit students from Courant CS for now.

For prospective students or interns who want to work with me in short term, please fill out [https://forms.gle/bfV5598RaBkrEvMi7 this form] so that we could find a suitable project for you. 

For prospective post-doc applicants, I encourage you to apply for the positions of [https://cds.nyu.edu/fellowsjobs/ CDS Faculty Fellows], [https://math.nyu.edu/dynamic/research/postdoctoral-and-visitor-programs/ Courant Instructors], and [https://www.simonsfoundation.org/grant/flatiron-research-fellow-opportunities/ Flatiron Research Fellows]. 

== News and Announcement
[08/2023] Won the [https://www.nyu.edu/content/dam/nyu/provost/documents/Research/2017_Whitehead_Fellowship.pdf Whitehead Fellowship for Junior Faculty in Biomedical and Biological Sciences]

[06/2023] New papers out:
- [https://arxiv.org/pdf/2307.11030 Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering]
- [https://arxiv.org/pdf/2306.12383 Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms]

[04/2023] Paper accepted at ICML 2023: 
-  Tianci Liu, Tong Yang, Quan Zhang, *Qi Lei*. [https://arxiv.org/abs/2210.13983 ``Optimization for Amortized Inverse Problems"]

[04/2023] Won the NYU Research Catalyst Prize jointly with [https://www.stern.nyu.edu/faculty/bio/zhengyuan-zhou Zhengyuan Zhou]

[01/2023] Four papers accepted at AISTATS 2023:
- Zihan Wang, Jason Lee, *Qi Lei*. [https://arxiv.org/abs/2212.03714 ``Reconstructing Training Data from Model Gradient, Provably"]
- Shuo Yang, Yijun Dong, Rachel Ward, Inderjit Dhillon, Sujay Sanghavi, *Qi Lei*. [https://arxiv.org/abs/2202.12230 ``Sample Efficiency of Data Augmentation Consistency Regularization"]
- Kurtland Chua, *Qi Lei*, Jason Lee. [https://arxiv.org/abs/2110.09507 ``Provable Hierarchy-Based Meta-Reinforcement Learning"]
- Qian Yu, Yining Wang, Baihe Huang, *Qi Lei*, Jason Lee. ``Optimal Sample Complexity Bounds for Non-convex Optimization under Kurdyka-Lojasiewicz Condition"

[01/2023] Invited talk at [https://slowdnn-workshop.github.io/schedule/ SLowDNN] on [https://arxiv.org/abs/2212.03714 reconstruction attack with guarantees]

[12/2022] Organized the [https://meta-learn.github.io/2022/ workshop on meta-learning] at NeurIPS 2022

[09/2022] Presented the a unified view on [https://arxiv.org/abs/2008.01064 reconstruction-based and similarity-based self-supervised learning] in [https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=74398 SIAM-MDS]

[09/2022] Started a new journey as an assistant professor of Courant Math/CDS at NYU!

#[05/2022] Presented my recent work on [https://cecilialeiqi.github.io/LP.pdf handling distribution shifts] and won *best poster award* at [http://zke.fas.harvard.edu/HawaiiConference/Main.html New Advances in Statistics and Data Science] 

#[04/2022] Invited talk on [https://cecilialeiqi.github.io/job_talk.pdf Theoretical foundations of Pre-trained Models] at AlgML seminar in Princeton 

#[04/2022] Invited talk at [https://math.dartmouth.edu/~acms/ Dartmouth ACMS] 

#[03/2022] New papers out:
#- [https://arxiv.org/abs/2202.12230 Sample Efficiency of Data Augmentation Consistency Regularization]
#- [https://arxiv.org/abs/2203.15664 Nearly Minimax Algorithms for Linear Bandits with Shared Representation]

#[02/2022] Invited talk at [https://simons.berkeley.edu/workshops/games2022-1 Adversarial Approaches in Machine Learning workshop] at Simons Institute

#[12/2021] Invited talk at the [https://drive.google.com/file/d/1wAEyNqGaY_GrnoUXXdSXbLlNdGIBLSu-/view USC Machine Learning Symposium] 

#[12/2021] Invited talk in the [https://www.ece.gatech.edu/calendar/day/2021/12/03/108208?utm_source=ECE+Lists+2019&utm_campaign=a5a1732812-EMAIL_CAMPAIGN_2019_08_06_08_30_COPY_01&utm_medium=email&utm_term=0_0324b14402-a5a1732812-103287103 CSIP seminar at Gatech]

#[11/2021] Invited talk in the [https://ellis.ist.ac.at/ ELLIS talk series] on [https://talks-calendar.app.ist.ac.at/events/3329 ``Provable representation learning"]

#[10/2021] I'm honored to be selected as a [https://ml.umd.edu/rising-stars rising star in Machine Learning] at the University of Maryland

#[10/2021] I'm honored to be selected as a [https://risingstars21-eecs.mit.edu/lei-qi/ rising star in EECS] at MIT

#[10/2021] I'm invited to give a talk on [https://simons.berkeley.edu/talks/optimal-gradient-based-algorithms-non-concave-bandit-optimization non-concave bandit optimization] at [https://simons.berkeley.edu/workshops/schedule/16590 the Sampling Algorithms and Geometries on Probability Distributions workshop] at Simons Institute

#[09/2021] All submissions got accepted at NeurIPS 2021!
#- Baihe Huang\*, Kaixuan Huang\*, Sham M. Kakade\*, Jason D. Lee\*, *Qi Lei*\*, Runzhe Wang\*, Jiaqi Yang\* [https://arxiv.org/abs/2107.04518 ``Optimal Gradient-based Algorithms for Non-concave Bandit Optimization"]
#- Baihe Huang\*, Kaixuan Huang\*, Sham M. Kakade\*, Jason D. Lee\*, *Qi Lei*\*, Runzhe Wang\*, Jiaqi Yang\* [https://arxiv.org/abs/2107.06466 ``Going Beyond Linear RL: Sample Efficient Neural Function Approximation] 
#- Kurtland Chua, *Qi Lei*, Jason D. Lee, [https://arxiv.org/abs/2105.02221 ``How fine-tuning allows for effective meta-learning"]
#- Jason D Lee\*, *Qi Lei*\*, Nikunj Saunshi\*, Jiacheng Zhuo\*, [https://arxiv.org/abs/2008.01064 ``Predicting What You Already Know Helps: Provable Self-Supervised Learning"]

#[09/2021] Invited talk at [http://bliss.eecs.berkeley.edu/Seminar/fa21/qi.html BLISS seminar]

#[07/2021] New papers out:
#- [https://arxiv.org/abs/2107.04518 Optimal Gradient-based Algorithms for Non-concave Bandit Optimization]
#- [https://arxiv.org/abs/2107.06466 Going Beyond Linear RL: Sample Efficient Neural Function Approximation]
#- [https://arxiv.org/abs/2107.02377 A Short Note on the Relationship of Information Gain and Eluder Dimension]

#[05/2021] Three papers are accepted at ICML 2021:
#- *Qi Lei*, Wei Hu, Jason D. Lee. [https://arxiv.org/abs/2106.12108 ``Near-Optimal Linear Regression under Distribution Shift"]
#- Tianle Cai\*, Ruiqi Gao\*, Jason D Lee\*, *Qi Lei*\*.  #[https://arxiv.org/abs/2102.11203 ``A Theory of Label Propagation for Subpopulation Shift"]
#- Jay Whang, *Qi Lei*, Alexandros G. Dimakis. #[https://arxiv.org/abs/2003.08089 ``Solving Inverse Problems with a Flow-based Noise Model"]

#[05/2021] I'm invited to give a talk on #[https://cms.caltech.edu/events/90169 Provable Representation Learning] at #[https://eas.caltech.edu/young_investigators Caltech Young Investigators Lecture]

#[05/2021] New paper out:
#[https://arxiv.org/abs/2105.02221  How Fine-Tuning Allows for Effective Meta-Learning]

#[03/2021] My PhD thesis [https://cecilialeiqi.github.io/LEI-DISSERTATION-2020.pdf Provably Effective Algorithms for Min-Max Optimization] won this year's [https://www.oden.utexas.edu/about/news/qi-lei-wins-Outstanding-Dissertation-Award/ Oden Institute Outstanding Dissertation Award] 

#[01/2021] Our paper is accepted at AISTATS 2021:
#- *Qi Lei*\*, Sai Ganesh Nagarajan\*, Ioannis Panageas\*, Xiao Wang\*. #"Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes"

#[01/2021] Our paper is accepted at ICLR 2021:
#- Simon S. Du\*, Wei Hu\*, Sham M. Kakade\*, Jason D. Lee\*, *Qi Lei*\*. #"Few-Shot Learning via Learning the Representation, Provably"

#[12/2020] I'm invited to give a talk on [https://simons.berkeley.edu/talks/sgd-learns-one-layer-networks-wgans SGD learns one-layer network with WGANs] at [https://simons.berkeley.edu/workshops/hd-2020-3 Learning and Testing in High Dimensions]

#[11/2020] I'm invited to give a Young Researcher Spotlight Talk at the [https://sites.google.com/view/slowdnn/ "Seeking Low-dimensionality in Deep Learning"] workshop on my recent work of [https://www.youtube.com/watch?v=SvddIPXjNDU&list=PL7P834wcUN3eSh7Nn4wzPBjEnme4Rls7n&index=15&ab_channel=SLowDNNWorkshop provable representation learning]

#[10/2020] I'm invited to give a talk on [https://www.youtube.com/watch?v=5A1iKkPv1Y0&feature=youtu.be&ab_channel=Oneworldtheoreticalmachinelearning provable self-supervised learning] at [https://www.oneworldml.org/ One-World ML seminar] 

#[10/2020] Our paper is accepted at NeurIPS 2020: 
#- Xiao Wang, *Qi Lei*, Ioannis Panageas.  "Fast Convergence of Langevin Dynamics on Manifold: Geodesics meet Log-Sobolev"

#[09/2020] I am selected as a 2020 Computing Innovation Fellow. Thank you CRA! 

#[08/2020] New paper out:
#- [https://arxiv.org/abs/2008.01064 "Predicting What You Already Know Helps: Provable Self-Supervised Learning"]

#[06/2020] Our paper is accepted at ICML 2020: 
#- *Qi Lei*, Jason D. Lee, Alexandros G. Dimakis, Constantinos Daskalakis. [https://arxiv.org/abs/1910.07030 "SGD Learns One-Layer Networks in WGANs"] 

#[03/2020] New papers out:
#- [https://arxiv.org/abs/2003.10392 "Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting"]
#- [https://arxiv.org/abs/2003.08089 "Compressed Sensing with Invertible Generative Models and Dependent Noise"]

#[02/2020] New papers out:
#- [https://arxiv.org/abs/2002.09434 "Few-Shot Learning via Learning the Representation, Provably"]
#- [https://arxiv.org/abs/2002.06789  "CAT: Customized Adversarial Training for Improved Robustness"]
#- [https://arxiv.org/abs/2002.06768  "Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes"]

#[01/2020] Our paper is accepted to AISTATS 2020:
#- Jiacheng Zhuo, *Qi Lei*, Alexandros G. Dimakis, Constantine Caramanis. [https://arxiv.org/abs/1910.07703 "Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Ball"]

#[12/2019] Attending [https://neurips.cc/ NeurIPS 2019] to present our following papers:
#- *Qi Lei*, Jiacheng Zhuo, Constantine Caramanis, Inderjit S Dhillon, Alexandros G Dimakis. [https://arxiv.org/abs/1906.02436 "Primal-Dual Block Generalized Frank-Wolfe"]
#- *Qi Lei*, Ajil Jalal, Inderjit S. Dhillon, Alexandros G. Dimakis. [https://arxiv.org/abs/1906.07437 "Inverting Deep Generative models, One layer at a time"]


#[11/2019] I am invited to give a talk on Deep Generative models and Inverse Problems at the mini-symposium "Machine Learning for Solving Partial Differential Equations and Inverse Problems" of the 2nd Annual Meeting of the [http://faculty.smu.edu/sxu/SIAMTXLA19/index.html SIAM Texas-Louisiana Section] in Dallas on Nov 2nd, 2019

#[10/2019] I am going to attend the [https://publish.illinois.edu/rising-stars/ Rising Stars 2019] at UIUC (An Academic Career Workshop for Women in EECS) from Oct 29th to Nov 1st, 2019

#[10/2019] New paper out: [https://arxiv.org/abs/1910.07703 "Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls"]

#[10/2019] New paper out: [https://arxiv.org/abs/1910.07030 "SGD Learns One-Layer Networks in WGANs"]

#[09/2019] I am participating the [https://www.math.ias.edu/sp/Optimization_Statistics_and_Theoretical_Machine_Learning "Special Year on Optimization, Statistics, and Theoretical Machine Learning"] as a short-term visitor at the Institute of Advanced Study from September to December, 2019

#[09/2019] Two papers accepted at NeurIPS 2019
#- *Qi Lei*, Jiacheng Zhuo, Constantine Caramanis, Inderjit S Dhillon, Alexandros G Dimakis. [https://arxiv.org/abs/1906.02436 "Primal-Dual Block Frank-Wolfe"]
#- *Qi Lei*, Ajil Jalal, Inderjit S. Dhillon, Alexandros G. Dimakis. [https://arxiv.org/abs/1906.07437 "Inverting Deep Generative models, One layer at a time"]

#[05/2019] I received [https://simons.berkeley.edu/programs/fellows Simons-Berkeley fellowship] in Foundations of Deep Learning, 2019.

#[04/2019] Our sysML paper on paraphrasing attacks [https://arxiv.org/abs/1812.00151  "Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification"] was covered by [https://www.nature.com/articles/d41586-019-01510-1?utm_source=twt_nnc&utm_medium=social&utm_campaign=naturenews&sf212595612=1 Nature Story], [https://venturebeat.com/2019/04/01/text-based-ai-models-are-vulnerable-to-paraphrasing-attacks-researchers-find VectureBeat], and [https://bdtechtalks.com/2019/04/02/ai-nlp-paraphrasing-adversarial-attacks/ TechTalks] 
#[https://github.com/cecilialeiqi/adversarial_text (code], [discrete_attack.pdf slides)] 




== Selected Papers

([papers.html full publication list])

7. Zihan Wang, Jason Lee, *Qi Lei*.
[https://arxiv.org/abs/2212.03714 "Reconstructing Training Data from Model Gradient, Provably"], AISTATS 2023

6. Baihe Huang\*, Kaixuan Huang\*, Sham M. Kakade\*, Jason D. Lee\*, *Qi Lei*\*, Runzhe Wang\*, and Jiaqi Yang\*.
[https://arxiv.org/abs/2107.04518 "Optimal Gradient-based Algorithms for Non-concave Bandit Optimization"], NeurIPS 2021

5. Jason D. Lee\*, *Qi Lei*\*, Nikunj Saunshi\*, Jiacheng Zhuo\*. [https://arxiv.org/abs/2008.01064 "Predicting What You Already Know Helps: Provable Self-Supervised Learning"], NeurIPS 2021

4. Simon S. Du\*, Wei Hu\*, Sham M. Kakade\*, Jason D. Lee\*, *Qi Lei*\*.  [https://arxiv.org/abs/2002.09434 "Few-Shot Learning via Learning the Representation, Provably"], /The International Conference on Learning Representations (ICLR) 2021/

3. *Qi Lei*, Jason D. Lee, Alexandros G. Dimakis, Constantinos Daskalakis. [https://arxiv.org/abs/1910.07030 "SGD Learns One-Layer Networks in WGANs"], /Proc. of International Conference of Machine Learning (ICML) 2020/

2. *Qi Lei*\*, Lingfei Wu\*, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. Dhillon, Michael Witbrock. [https://arxiv.org/abs/1812.00151 "Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification"], /Systems and Machine Learning (sysML). 2019/ [https://github.com/cecilialeiqi/adversarial_text (code], [discrete_attack.pdf slides)] 
- Press coverage: [https://www.nature.com/articles/d41586-019-01510-1?utm_source=twt_nnc&utm_medium=social&utm_campaign=naturenews&sf212595612=1 <Nature Story>] [https://venturebeat.com/2019/04/01/text-based-ai-models-are-vulnerable-to-paraphrasing-attacks-researchers-find/ <Vecturebeat>] [https://bdtechtalks.com/2019/04/02/ai-nlp-paraphrasing-adversarial-attacks/ <Tech Talks>] [https://www.jiqizhixin.com/articles/2019-03-27-10?from=synced&keyword=SysML%202019 <机器之心>]


1. Rashish Tandon, *Qi Lei*, Alexandros G. Dimakis, Nikos Karampatziakis, [https://arxiv.org/abs/1612.03301 "Gradient Coding: Avoiding Stragglers in Distributed Learning"], /Proc. of International Conference of Machine Learning (ICML), 2017/ [https://github.com/rashisht1/gradient_coding (code)]




