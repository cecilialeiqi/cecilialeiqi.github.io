%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Medium Length Graduate Curriculum Vitae
% LaTeX Template
% Version 1.1 (9/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Rensselaer Polytechnic Institute (http://www.rpi.edu/dept/arc/training/latex/resumes/)
%
% Important note:
% This template requires the res.cls file to be in the same directory as the
% .tex file. The res.cls file provides the resume style used for structuring the
% document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[margin, 10pt]{res} % Use the res.cls style, the font size can be changed to 11pt or 12pt here
\usepackage{hyperref}

\usepackage{helvet} % Default font is the helvetica postscript font
%\usepackage{newcent} % To change the default font to the new century schoolbook postscript font uncomment this line and comment the one above
\usepackage{enumitem}
\usepackage{url}
\usepackage{comment}
\setlength{\textwidth}{5.1in} % Text width of the document

\begin{document}

%----------------------------------------------------------------------------------------
%	NAME AND ADDRESS SECTION
%----------------------------------------------------------------------------------------

\moveleft.5\hoffset\centerline{\large\bf Lei, Qi} % Your name at the top
 
\moveleft\hoffset\vbox{\hrule width\resumewidth height 1pt}\smallskip % Horizontal line after name; adjust line thickness by changing the '1pt'
 
\moveleft.5\hoffset\centerline{Website: \url{https://cecilialeiqi.github.io/}}
\moveleft.5\hoffset\centerline{Google Scholar: \url{https://scholar.google.com/citations?user=kGOgaowAAAAJ&hl=en}}
\moveleft.5\hoffset\centerline{Email: \url{qilei@princeton.edu}}
%----------------------------------------------------------------------------------------

\begin{resume}

%----------------------------------------------------------------------------------------
%	OBJECTIVE SECTION
%----------------------------------------------------------------------------------------
 
%\section{RESEARCH\\ INTERESTS}  
%\textbf{Machine Learning:} Provable, efficient and robust algorithms for 
%fundamental machine learning problems, distributed and parallel learning, 
%min-max optimizations with applications to generative adversarial networks and adversarial training

%\textbf{Large-Scale Optimization:} Novel algorithms that exploit 
%the underlying problem structure and are scalable to massive data sets

%\textbf{Deep Learning:} 


%My research interests mainly focus on machine learning, numerical optimization and their applications, such as matrix completion and recommendation system.
%To obtain a career that will allow myself to take full advantage of my passion and experience in software engineering and computer science. 

%----------------------------------------------------------------------------------------
%	EDUCATION SECTION
%----------------------------------------------------------------------------------------




\section{Education}
{\sl University of Texas at Austin, TX, United States} \hfill August 2014 - May 2020
\begin{itemize}
\item {\sl Ph.D., Oden Institute for Computational Sciences and Engineering} 
\item Advisors: Alexandros 
  G. Dimakis  and Inderjit S. Dhillon
\end{itemize} 



{\sl Zhejiang University, Zhejiang, China} \hfill Sep 2010 - June 2014 \begin{itemize}
  \item {\sl B.S., School of Mathematics (with honors)} (GPA 3.92/4.0, rank $1^{st}$)
%  \item Advisor: Qunsheng Peng, State Key Lab of CAD\&CG.
%\item[--] Related courses: Computer Graphics, Discrete Mathematics, Combinatorics, Object-Oriented Programming, Lab\& Fundamentals of C Programming, Scientific Computing, Fundamentals of Logic and Computer Design
\end{itemize}

\section{Research\\ Interests}
My research aims to bridge the theoretical and empirical boundary of modern machine learning algorithms, with a focus on large-scale optimization and representation/transfer learning. 
\begin{comment} 
\textbf{Large-scale optimization:}
\begin{itemize}
	\item Efficient and scalable optimization algorithm design: exploiting problem's underlying structure
	\item Two-player games: customized adversarial robustness and generative adversarial network training 
	\item Distributed systems: gradient coding and asynchronous distributed learning 
\end{itemize}
\textbf{Representation Learning and Transfer Learning}
\begin{itemize}
\item Statistical learning theory of representation with supervised pretraining, fine-tuning and self-supervised learning
%\item Domain adaptation with covariate shift and through label propagation
\item Application to meta-reinforcement learning 
\end{itemize}
Applications:
\begin{itemize}
	\item Inverse problems with generative models
\end{itemize}
\end{comment} 

\section{Professional\\Experience}
{\sl Princeton University, NJ, United States} \hfill July 2020 - Present
\begin{itemize}
	\item {\sl Associate Research Scholar (\href{https://cifellows2020.org/}{CIFellow})}, Electrical and Computer Engineering Department \hfill September 2021 - Present 
	\item {\sl Postdoc Research Associate (\href{https://cifellows2020.org/}{CIFellow})}, Electrical and Computer Engineering Department \hfill July 2020 - September 2021 
	\item Mentor: Jason D. Lee
\end{itemize}


{\sl Institute for Advanced Study, Princeton, NJ, United States}\\
\hspace*\fill\hfill September 2019 - July 2020
\begin{itemize}
	\item Visiting Graduate Student for the ``Special Year on Optimization, Statistics, and Theoretical Machine Learning''
\end{itemize}

{\sl Simons Institute, Berkeley, CA, United States} \hfill May 2019 - August 2019
\begin{itemize}
	\item Research Fellow for the Foundations of Deep Learning Program
\end{itemize}



\section{Awards and Recognitions}\begin{itemize}[noitemsep]
	\item {Computing Innovation Fellowship (\$150k) } \hspace*\fill\hfill{CRA, 2020-2022}
	\item {Simons-Berkeley Research Fellowship} \hspace*\fill\hfill{Simons Institute, 2019}
	\item { The National Initiative for Modeling and Simulation Research 
		Fellowship   (\$225k)}\\  \hspace*\fill\hfill{UT Austin, 2014-2018}
	\item {Young Investigators Lecturer award} \hspace*\fill\hfill{Caltech, 2021}
	\item { Outstanding Dissertation Award}\hspace*\fill\hfill{Oden Institute, 2020}
	\item {Dissertation Writing Fellowship} \hspace*\fill\hfill{UT Austin, 2021}
	\item {Rising Star for Machine Learning} \hspace*\fill\hfill{University of Maryland, 2021}
	\item {Rising Star for EECS} \hspace*\fill\hfill{UIUC, 2019 \& MIT, 2021}
	\item {Rising Star for Computational and Data Science} \hspace*\fill\hfill{UT Austin, 2020}
	%\item {Gold medal ($5^{th}$ place) in China Girls Math Olympiad (CGMO, an international competition with a proof-based format similar to the International Math Olympiad)} \hfill{China, 2009}
%	\item {Meritorious Winner for The Mathematical Contest in Modeling (MCM) }\\
	%\hspace*\fill{COMAP, 2014}
	\item {The Excellence Scholarship (top honor)} \hfill{Zhejiang Univ, 2014}
	%\item {First Prize for CMC (the Mathematics competition of Chinese College Student)} \hspace*\fill\hfill{China, 2012}
	%\item {First Prize Scholarship \& Merit Student} \hfill{Zhejiang Univ, 2010-2014}
	%\item {Third Prize for ACM Programming Contest} \hfill{Zhejiang Univ, 2012}
	%\item {First Prize for National Olympiad in Informatics in Provinces (NOIP)}\\
	%\hspace*\fill \hfill{China, 2007(perfect score), 2008}
\end{itemize}






%\section{SOFTWARE}

%----------------------------------------------------------------------------------------
%	INDUSTRY EXPERIENCE
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%	Technology SKILLS SECTION
%----------------------------------------------------------------------------------------

\section{Thesis} 
\textbf{Qi Lei}, ``Provably effective algorithms for min-max optimization" \hfill{May 2020} \\ 
\textit{Received the 2021 Outstanding Dissertation Award, Oden Institute}


\section{Publications\\
	\vspace{4pt}
{\footnotesize ($*$ indicates\\ $\alpha$-$\beta$ order) }}
\begin{enumerate}
		\item {Jason D. Lee*, \textbf{Qi Lei}*, Nikunj Saunshi*, Jiacheng Zhuo*, ``Predicting What You Already Know Helps: Provable Self-Supervised Learning”, \textit{to appear at NeurIPS 2021}}


\item {Baihe Huang*, Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*, Runzhe Wang*, Jiaqi Yang*,  ``Optimal Gradient-based Algorithms for Non-concave Bandit Optimization", 	\textit{to appear at NeurIPS 2021} }


\item 	{ Kurtland Chua, \textbf{Qi Lei}, Jason D. Lee. ``How Fine-Tuning Allows for Effective Meta-Learning”, \textit{to appear at NeurIPS 2021} }



\item {Baihe Huang*, Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*, Runzhe Wang*, Jiaqi Yang*, ``Going Beyond Linear RL: Sample Efficient Neural Function Approximation", \textit{to appear at NeurIPS 2021} }
	
	
\item{ \textbf{Qi Lei}, Wei Hu, Jason D. Lee.	
``Near-Optimal Linear Regression under Distribution Shift", \textit{ International Conference of Machine Learning (ICML), 2021: 6164-6174}
}	
\item{Tianle Cai*, Ruiqi Gao*, Jason D Lee*, \textbf{Qi Lei}*.  ``A Theory of Label Propagation for Subpopulation Shift", \textit{International Conference of Machine Learning (ICML), 2021: 1170-1182
} }
\item{Jay Whang, \textbf{Qi Lei}, Alexandros G. Dimakis. ``Solving Inverse Problems with a Flow-based Noise Model", \textit{International Conference of Machine Learning (ICML), 2021: 11146-11157
 }}
	\item{Simon S. Du*, Wei Hu*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*. ``Few-Shot Learning via Learning the Representation, Provably'', \textit{International Conference on Learning Representations, 2021} }
	\item{\textbf{Qi Lei}*, Sai Ganesh Nagarajan*, Ioannis Panageas*, Xiao Wang*. ``Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes'',\textit{International Conference on
			Artificial Intelligence and Statistics, 2021: 1441-1449}}
	\item{ Xiao Wang, \textbf{Qi Lei}, Ioannis Panageas. ``Fast Convergence of Langevin Dynamics on Manifold: Geodesics meet Log-Sobolev'', \textit{Proc. of Neural Information Processing Systems (NeurIPS), 2020 }  }
%\item{Jason D. Lee*, \textbf{Qi Lei}*, Nikunj Saunshi*, Jiacheng Zhuo*. ``Predicting What You Already Know Helps: Provable Self-Supervised Learning'', \textit{NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice} }
%\item{Jay Whang, \textbf{Qi Lei}, Alexandros G. Dimakis. ``Compressed Sensing with Invertible Generative Models and Dependent Noise'', \textit{NeurIPS 2020 Workshop on Deep Learning and Inverse Problems}} 	
\item{Jiacheng Zhuo, \textbf{Qi Lei}, Alexandros G. Dimakis, Constantine 
      Caramanis.\\ ``Communication-Efficient Asynchronous Stochastic 
    Frank-Wolfe over Nuclear-norm Ball'', \textit{The 23rd International Conference on Artificial Intelligence and Statistics, 2020: 1464-1474} }
\item{\textbf{Qi Lei}, Jason Lee, Alexandros G. Dimakis, Contantinos 
  Daskalakis. \\ ``SGD Learns One-Layer Networks in WGANs'', 
    \textit{International Conference of Machine Learning (ICML), 2020: 5799-5808}}
  \item{ \textbf{Qi Lei}, Jiacheng Zhuo, Constantine Caramanis, Inderjit S. 
    Dhillon, Alexandros G. Dimakis. ``Primal-Dual Block Generalized Frank-Wolfe'', 
  \textit{Proc. of Neural Information Processing Systems (NeurIPS), 2019: 13866-13875} }
  \item{ \textbf{Qi Lei}, Ajil Jalal, Inderjit S. Dhillon, Alexandros G. 
      Dimakis. ``Inverting Deep Generative models, One layer at a time'', 
    \textit{Proc. of Neural Information Processing Systems (NeurIPS), 2019: 13910-13919} }

  \item{ \textbf{Qi Lei}, Jinfeng Yi, Roman Vaculin, Lingfei Wu, Inderjit S. 
      Dhillon. ``Similarity Preserving Representation Learning for Time Series 
      Analysis'', \textit{The 28th International Joint Conference on Artificial 
    Intelligence (IJCAI), 2019: 2845-2851}}
  \item{\textbf{Qi Lei}, Lingfei Wu, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. 
    Dhillon, Michael Witbrock. ``Discrete Adversarial Attacks and Submodular 
    Optimization with Applications to Text Classification'', \textit{Systems and Machine 
  Learning (MLsys), 2019} (\textbf{covered by \href{https://www.nature.com/articles/d41586-019-01510-1}{Nature News} )  } }
\item{Jinfeng Yi, \textbf{Qi Lei}, Wesley M Gifford, Ji Liu, Junchi Yan, Bowen Zhou. ``Fast Unsupervised Location Category Inference from Highly Inaccurate Mobility Data'', \textit{Proceedings of the 2019 SIAM International Conference on Data Mining 2019: 55-63}}
\item{Zhewei Yao, Amir Gholami, \textbf{Qi Lei}, Kurt Keutzer, Michael W. 
  Mahoney. ``Hessian-based Analysis of Large Batch Training and Robustness to 
Adversaries'', \textit{Neural Information Processing Systems (NIPS), 2018: 4954-4964}}
  \item{Jiong Zhang, \textbf{Qi Lei}, Inderjit S. Dhillon. 
        ``Stabilizing Gradients for Deep 
       Neural Networks via Efficient SVD Parameterization'', \textit{
    International Conference of Machine Learning (ICML), 2018: 5801-5809}}
\item{Jinfeng Yi, \textbf{Qi Lei}, Junchi Yan, Wei Sun. ``Session expert: A lightweight conference session recommender system'', \textit{IEEE International Conference on Big Data (Big Data), 2018: 1677-1682}}
  \item{Lingfei Wu, Ian En-Hsu Yen, Jinfeng Yi, Fangli Xu, \textbf{Qi Lei}, Michael Witbrock.
    ``Random Warping Series: A Random Features Method for Time-Series Embedding'', \textit{AISTATS 2018: 793-802}}
\item{Hsiang-fu Yu, Cho-Jui Hsieh, \textbf{Qi Lei}, Inderjit S. Dhillon. 
      ``A Greedy Approach for Budgeted Maximum 
      Inner Product Search'', \textit{Neural Information Processing Systems 
      (NIPS), 2017: 5453-5462}}
    \item{\textbf{Qi Lei}, Enxu Yen, Chao-yuan Wu, Inderjit S. Dhillon, Pradeep 
        Ravikumar. ``Doubly Greedy Primal-Dual Coordinate Methods on Sparse Empirical 
  Risk Minimization'', \textit{International Conference of Machine 
    Learning (ICML), 2017: 2034-2042}}
\item{Rashish Tandon, \textbf{Qi Lei}, 
    Alexandros G. Dimakis, Nikos Karampatziakis, ``Gradient Coding: Avoiding 
    Stragglers in Distributed Learning'', \textit{International Conference of 
  Machine Learning (ICML), 2017: 3368-3376}}
  \item {\textbf{Qi Lei},
      Kai Zhong, Inderjit S. Dhillon. ``Coordinate-wise Power Method'', 
  \textit{Neural Information Processing System(NIPS), 2016: 2056-2064}}		

%\item {\textbf{Qi Lei}, Jinfeng Yi, Roman Vaculin, Inderjit Dhillon. "Similarity Preserving Representation Learning for Time Series Analysis", \textit{Submitted for publication}}
%\item {Rashish Tandon, \textbf{Qi Lei}, Alexandros G. Dimakis, Nikos  Karampatziakis. "Gradient Coding", \textit{ML Systems Workshop at NIPS, Dec. 2016}}
	\item {Arnaud Vandaele, Nicolas Gillis, \textbf{Qi Lei},
      Kai Zhong, Inderjit S. Dhillon. ``Coordinate Descent Methods for
		Symmetric Nonnegative Matrix Factorization'', \textit{IEEE Transactions on 
Signal Processing}, 64.21 (2016): 5571-5584}	
%	\item {Hsiang-Fu Yu, Cho-Jui Hsieh, \textbf{Qi Lei}, Inderjit S. Dhillon. "A Greedy Approach for Budgeted Maximum Inner Product Search", \textit{arXiv:1610.03317v1}}

	\item {Maria R. D'Orsogna, \textbf{Qi Lei}, Tom Chou, ``First assembly times and equilibration in stochastic coagulation-fragmentation", \textit{The Journal of Chemical Physics}, 2015: 143.1, 014112}
		\item {Jiazhou Chen, \textbf{Qi Lei}, Yongwei Miao, Qunsheng Peng, ``Vectorization of Line Drawing Image based on Junction Analysis", \textit{Science China Information Sciences}, 2014:1-14}
	\item {Jiazhou Chen, \textbf{Qi Lei}, Fan Zhong, Qunsheng Peng, ``Interactive Tensor Field Design Based on Line Singularities", \textit{Proceedings of the 13th International CAD /Graphics}, 2013}
\end{enumerate}



\section{Workshop\\Articles}
\begin{enumerate}
	\item{Tianci Liu, Quan Zhang, \textbf{Qi Lei}, ``PANOM: Automatic Hyper-parameter Tuning for Inverse Problems", \textit{to appear at NeurIPS 2021 Workshop on Deep Learning and Inverse Problems} }
	\item {Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*,  ``A Short Note on the Relationship of Information Gain and Eluder Dimension", \textit{ICML 2021 Workshop on Reinforcement Learning Theory} }
	\item {Jason D. Lee*, \textbf{Qi Lei}*, Nikunj Saunshi*, Jiacheng Zhuo*, ``Predicting What You Already Know Helps: Provable Self-Supervised Learning”, \textit{NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice}}
	
\item{Jay Whang, \textbf{Qi Lei}, Alex Dimakis, 	``Compressed Sensing with Invertible Generative Models and Dependent Noise", \textit{NeurIPS 2020 Workshop: Deep Learning and Inverse Problems}
}

\item{\textbf{Qi Lei}, Ajil Jalal, Inderjit Dhillon, Alexandros Dimakis, ``Inverting Deep Generative models, One layer at a time", \textit{ICML 2019 Workshop on Invertible Neural Networks and Normalizing Flows} 	}
	
\item{Rashish Tandon, \textbf{Qi Lei}, Alexandros G. Dimakis, Nikos Karampatziakis,	``Gradient Coding", \textit{NIPS 2016 Workshop on ML Systems (MLSys)} 
}
	
\end{enumerate} 

\section{Under\\Review\\Preprints}
\begin{enumerate}

\item{Kurtland Chua, \textbf{Qi Lei}, Jason D Lee. ``Provable Hierarchy-Based Meta-Reinforcement Learning", \textit{arXiv preprint} }

\item{Lemeng Wu, Mao Ye, \textbf{Qi Lei}, Jason D. Lee, and Qiang Liu. ``Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting”, \textit{arXiv preprint}}

\item{ Minhao Cheng, \textbf{Qi Lei}, Pin-Yu Chen, Inderjit Dhillon, Cho-Jui Hsieh. ``CAT: Customized Adversarial Training for Improved Robustness”, \textit{arXiv preprint}}


\end{enumerate} 


\section{Patents}\begin{itemize}
	\item{``Method and System for General and Efficient Time Series Representation 
		Learning via Dynamic Time Warping."\\
		\textbf{Q. Lei}, J. Yi, R. Vaculin, and W. Sun}
	
	\item{``Real-Time Cold Start Recommendation and Rationale within a Dialog System".\\
		\textbf{Q. Lei}, J. Yi, R. Vaculin, M. Pietro}
\end{itemize}
%----------------------------------------------------------------------------------------
%	PROFESSIONAL EXPERIENCE SECTION
%----------------------------------------------------------------------------------------
\begin{comment}
\section{SOFTWARE}
 \textit{Github: }\url{https://github.com/cecilialeiqi/}\\
 {\sl SPIRAL} \hfill May 2016 - July 2017
 \begin{itemize}
   \item Feature representation learning of any time series data
    \end{itemize}
 {\sl DDI} \hfill Jan 2017 - May 2017
 \begin{itemize}
   \item Use an inductive tensor completion based methods to predict drug-drug interactions
   \end{itemize}
   {\sl NUTF} \hfill August 2016 - February 2017
   \begin{itemize}
     \item Negative-Unlabeled Tensor Factorization for Location\ Context 
       Inference from Inaccurate Mobility Data
      \end{itemize}
 \end{itemize} 
\end{comment}

\section{Teaching}
{\sl Department of Electrical and Computer Engineering, Princeton} \hfill Fall 2020
\begin{itemize}
	\item Theory of Deep Learning: Representation and Weakly Supervised Learning: {\sl Teaching Assistant}
\end{itemize}


{\sl Department of Electrical and Computer Engineering, UT Austin} \hfill Fall 2019
\begin{itemize}
  \item Scalable Machine Learning: {\sl Teaching Assistant}
  \end{itemize}

{\sl Oden Institute for Computational Engineering and Sciences, UT Austin} \hfill Fall 2015
\begin{itemize}
  \item Mathematical Methods in Applied Engineering and Sciences: {\sl Instructor Intern}
\end{itemize}


\section{Internships}

{\sl Facebook/Photo\&Video Search} \hfill June 2018 - September 2018
\begin{itemize}
	\item Explored offline/online evaluation gaps by estimating expected number of 
	clicks based on historical logging data.
\end{itemize}
{\sl Amazon/A9 Product Search} \hfill May 2017 - August 2017
\begin{itemize}
	\item Inline search suggestions: used deep learning methods for NLP user 
	search tasks.
\end{itemize}
{\sl Amazon Web Services (AWS Deep Learning Team)} \hfill January 2017 - April 2017
\begin{itemize}
	\item Documentations for MXNet: a deep learning framework designed for 
	both efficiency and flexibility.
\end{itemize}
{\sl IBM Thomas J. Watson Research Center} \hfill May 2016 - October 2016
\begin{itemize}
	\item  %Clients' propensity prediction of trading options
	Partnered with one of the largest American financial companies on a challenge problem of predicting its clients' propensity of trading options
	\item Create World of Watson Session recommendation system:\\
	\url{https://myibm.ibm.com/events/wow/watson/}
	%\item 
\end{itemize} 



\section{Service}
Co-organizing Mathematical Data Science Reading Group, which is a weekly departmental seminar series on Machine Learning Theory in ECE, Princeton, 2020 

Student mentor (for a female Ph.D. student),  Oden Institute, 2018
%{\sl 2021:} Serve as a mentor at the mentorship program of WiML-T (Women in Machine Learning Theory)

{\sl Conference Reviewer:} MLSys (19,20,Meta-reviewer'21, 22), COLT (21,22), STOC (20), NeurIPS (16,17,18,19,20,21), ICML (18,19,20,21), ICLR (18,19,20,21), 
AISTATS (18,19,20,21), AAAI (20,21), ACML (19), and more

{\sl Journal Reviewer:} JSAIT(20), MOR (18,19,20), TNNLS (19,20), TKDE (19), ISIT (17,18), TIIS (17), 
IT (16,17), and more




 \section{Invited Talks}
{``Optimal Gradient-based Algorithms for Non-concave Bandit Optimization."}
 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 		\item BLISS seminar, UC Berkeley, virtual 2021
 		\item Sampling Algorithms and Geometries on Probability Distributions Workshop, Simons Institute, CA, 2021 
 	\end{itemize}
{`` Few-Shot Learning via Learning the Representation, Provably.''
		\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
			\item International Conference on Learning Representations, virtual, 2021
			\item 	IAS, Princeton, NJ, 2020 
			\item  Simons Institute Reunion, virtual, 2020 
			\item UC Berkeley, virtual, 2020	
		\end{itemize}
}
{``Predicting What You Already Know Helps: Provable Self-Supervised Learning.''
	 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
	 		\item Proc. of Neural Information Processing Systems, virtual, 2021
			\item Institute for Foundations of Machine Learning, virtual, 2020
			\item One-World ML seminar, virtual, 2020
	 		\item UW-Madison, virtual, 2020	 
	 	\end{itemize}
}
{``Provable representation learning.''
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
	\item 		Young Researcher Spotlight Talk at ``Seeking Low-dimensionality in Deep Learning'' workshop, virtual, 2020 
	\item  Microsoft Research, virtual, 2021  
	\item \href{https://cms.caltech.edu/events/90169}{Caltech Young Investigators Lecture Series}, virtual, 2021
	\item Rising star presentation at the University of Maryland, virtual, 2021
	\item  ELLIS Talk at IST Austria, virtual, 2021
	\item CSIP seminar at Georgia Tech, virtual, 2021
\end{itemize}
	 }
{``SGD Learns One-Layer Networks in WGANs.''
  	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
  		\item International Conference of Machine Learning (ICML), virtual, 2020
  		\item Workshop on Learning and Testing in High Dimensions, Simons Institute, 2020
  	\end{itemize} 
  }
  \newpage 
{``Deep Generative models and Inverse Problems.''
	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
		\item        Minisymposium on Machine Learning for Solving Partial Differential 
		Equations and Inverse Problems, 2019 SIAM Texas-Louisiana Section, Dallas,
		TX, USA, 2019 
		\item Princeton, virtual, 2020 
		\item  Google Research, virtual, 2021
	\end{itemize}
}


{``Similarity Preserving Representation Learning for Time Series Analysis.''
	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
		\item The 28th International Joint Conference on Artificial Intelligence (IJCAI), Macao, 2019
	\end{itemize}
}
{``Discrete Adversarial Attacks 
 and Submodular Optimization with Applications to Text Classification.''
 \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 	\item  Simons-Berkeley Fellows Talk, Berkeley, CA, USA 
 	\item The Conference on Systems and Machine Learning  (SysML), Stanford, CA, 
 	USA, 2019
 \end{itemize} 
}
{``Recent 
     Advances in Primal-Dual Coordinate Methods for ERM.''
     \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
     	\item      Minisymposium on Recent Progress in Coordinate-wise Descent Methods, SIAM Conference on Computational Science and Engineering, Spokane, WA, USA, 2019
     	\item  International Conference of Machine Learning (ICML), Sydney, 2017  
     \end{itemize}
}
{``Coordinate Descent Methods for Matrix Factorization.''
 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 		\item    Minisymposium on Recent Advances in Nonnegative Matrix Factorization, SIAM 
 		Annual Meeting, Boston, USA, 2016
 	\end{itemize}
}


% \end{item}
\begin{comment} 
\section{Programming \\ Skills} 
\begin{itemize}
	\item[]  C/C++(proficient), Python(proficient), Matlab(proficient), C\#(prior experience)
  \item[]  Familiar with Deep Learning packages(Pytorch, Tensorflow, Theano, MXNet)
\end{itemize}
% consider add software?
\end{comment} 
\end{resume}
\end{document}
