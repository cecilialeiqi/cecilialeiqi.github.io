%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Medium Length Graduate Curriculum Vitae
% LaTeX Template
% Version 1.1 (9/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Rensselaer Polytechnic Institute (http://www.rpi.edu/dept/arc/training/latex/resumes/)
%
% Important note:
% This template requires the res.cls file to be in the same directory as the
% .tex file. The res.cls file provides the resume style used for structuring the
% document.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[margin, 10pt]{res} % Use the res.cls style, the font size can be changed to 11pt or 12pt here
\usepackage{hyperref}

\usepackage{helvet} % Default font is the helvetica postscript font
%\usepackage{newcent} % To change the default font to the new century schoolbook postscript font uncomment this line and comment the one above
\usepackage{enumitem}
\usepackage{url}
\usepackage{comment}
\setlength{\textwidth}{5.1in} % Text width of the document

\begin{document}

%----------------------------------------------------------------------------------------
%	NAME AND ADDRESS SECTION
%----------------------------------------------------------------------------------------

\moveleft.5\hoffset\centerline{\large\bf Lei, Qi} % Your name at the top
 
\moveleft\hoffset\vbox{\hrule width\resumewidth height 1pt}\smallskip % Horizontal line after name; adjust line thickness by changing the '1pt'
 
\moveleft.5\hoffset\centerline{Website: \url{https://cecilialeiqi.github.io/}}
\moveleft.5\hoffset\centerline{Google Scholar: \url{https://scholar.google.com/citations?user=kGOgaowAAAAJ&hl=en}}
\moveleft.5\hoffset\centerline{Email: \url{qilei@princeton.edu}}
%----------------------------------------------------------------------------------------

\begin{resume}

%----------------------------------------------------------------------------------------
%	OBJECTIVE SECTION
%----------------------------------------------------------------------------------------
 
%\section{RESEARCH\\ INTERESTS}  
%\textbf{Machine Learning:} Provable, efficient and robust algorithms for 
%fundamental machine learning problems, distributed and parallel learning, 
%min-max optimizations with applications to generative adversarial networks and adversarial training

%\textbf{Large-Scale Optimization:} Novel algorithms that exploit 
%the underlying problem structure and are scalable to massive data sets

%\textbf{Deep Learning:} 


%My research interests mainly focus on machine learning, numerical optimization and their applications, such as matrix completion and recommendation system.
%To obtain a career that will allow myself to take full advantage of my passion and experience in software engineering and computer science. 

%----------------------------------------------------------------------------------------
%	EDUCATION SECTION
%----------------------------------------------------------------------------------------


\section{Research\\ Interests}
My research aims to bridge the theoretical and empirical boundary of modern machine learning algorithms and in particular AI safety, with a focus on data privacy, distributionally robust algorithms, sample- and parameter-efficient learning. 

\begin{comment} 
\textbf{Large-scale optimization:}
\begin{itemize}
	\item Efficient and scalable optimization algorithm design: exploiting problem's underlying structure
	\item Two-player games: customized adversarial robustness and generative adversarial network training 
	\item Distributed systems: gradient coding and asynchronous distributed learning 
\end{itemize}
\textbf{Representation Learning and Transfer Learning}
\begin{itemize}
\item Statistical learning theory of representation with supervised pretraining, fine-tuning and self-supervised learning
%\item Domain adaptation with covariate shift and through label propagation
\item Application to meta-reinforcement learning 
\end{itemize}
Applications:
\begin{itemize}
	\item Inverse problems with generative models
\end{itemize}
\end{comment} 

\section{Professional\\Experience}
{\sl New York University, NY, United States} \hfill September 2022 - 
\begin{itemize}
	\item {\sl Assistant Professor in Mathematics and Data Science},  Courant Institute of Mathematical Sciences and the Center for Data Science 
\end{itemize} 

{\sl Princeton University, NJ, United States} \hfill July 2020 - August 2022
\begin{itemize}
	\item {\sl Associate Research Scholar (\href{https://cifellows2020.org/}{CIFellow})}, Electrical and Computer Engineering Department \hfill September 2021 - Present 
	\item {\sl Postdoc Research Associate (\href{https://cifellows2020.org/}{CIFellow})}, Electrical and Computer Engineering Department \hfill July 2020 - September 2021 
	\item Mentor: \href{https://jasondlee88.github.io/}{Jason D. Lee}
\end{itemize}


{\sl Institute for Advanced Study, Princeton, NJ, United States}\\
\hspace*\fill\hfill September 2019 - July 2020
\begin{itemize}
	\item Visiting Graduate Student for the \href{https://www.ias.edu/math/sp/Optimization_Statistics_and_Theoretical_Machine_Learning}{``Special Year on Optimization, Statistics, and Theoretical Machine Learning''}
\end{itemize}

{\sl Simons Institute, Berkeley, CA, United States} \hfill May 2019 - August 2019
\begin{itemize}
	\item Research Fellow for the \href{https://simons.berkeley.edu/programs/dl2019}{Foundations of Deep Learning Program}
\end{itemize}




\section{Education}
{\sl University of Texas at Austin, TX, United States} \hfill August 2014 - May 2020
\begin{itemize}
	\item {\sl Ph.D., \href{https://oden.utexas.edu/}{Oden Institute for Computational Sciences and Engineering}} 
	\item Advisors: \href{https://users.ece.utexas.edu/~dimakis/}{Alexandros 
		G. Dimakis}  and \href{https://www.cs.utexas.edu/~inderjit/}{Inderjit S. Dhillon}
\end{itemize} 



{\sl Zhejiang University, Zhejiang, China} \hfill Sep 2010 - June 2014 \begin{itemize}
	\item {\sl B.S., School of Mathematics (with honors)} (GPA 3.92/4.0, rank $1^{st}$)
	%  \item Advisor: Qunsheng Peng, State Key Lab of CAD\&CG.
	%\item[--] Related courses: Computer Graphics, Discrete Mathematics, Combinatorics, Object-Oriented Programming, Lab\& Fundamentals of C Programming, Scientific Computing, Fundamentals of Logic and Computer Design
\end{itemize}

\section{Grants and Award\\ (Faculty)}\begin{itemize}[noitemsep]
	\item {\href{https://www.nyu.edu/about/leadership-university-administration/office-of-the-president/office-of-the-provost/support-for-research-at-nyu/institutional-support-for-faculty.html#RCP}{NYU Research Catalyst Prize}}, `Towards Data-Driven Resilient Supply Chains: A Statistical Foundation of Learning Robust Inventory Control Policies`" Co-PI, 15k \hspace*\fill\hfill{NYU,2023}
	\item {\href{https://www.nyu.edu/about/leadership-university-administration/office-of-the-president/office-of-the-provost/support-for-research-at-nyu/institutional-support-for-faculty.html}{Whitehead Fellowship for Junior Faculty in Biomedical and Biological Sciences}},``Provably Privacy-preserving Learning Pipelines for Sensitive Health Data", PI, 30k \hspace*\fill\hfill{NYU,2023}
\item
{\href{https://rome.cims.nyu.edu/about/}{Award \#DE-SC0024721}}, ``Learning reduced models under extreme data
conditions for design and rapid decision-making in complex systems", Co-PI, 3.97M \hspace*\fill\hfill{U.S. Department of Energy, Office of Science
	Energy Earthshot Initiative, 2023}
\end{itemize} 

\section{Selected Grants and Awards\\(pre-faculty)}\begin{itemize}[noitemsep]
%	\item {\href{https://www.nyu.edu/about/leadership-university-administration/office-of-the-president/office-of-the-provost/support-for-research-at-nyu/institutional-support-for-faculty.html#RCP}{NYU Research Catalyst Prize}}\hspace*\fill\hfill{NYU,2023}
%	\item {\href{https://www.nyu.edu/about/leadership-university-administration/office-of-the-president/office-of-the-provost/support-for-research-at-nyu/institutional-support-for-faculty.html}{Whitehead Fellowship for Junior Faculty in Biomedical and Biological Sciences}}\hspace*\fill\hfill{NYU,2023}
	\item {\href{https://cifellows2020.org/2020-class/}{Computing Innovation Fellowship} } \hspace*\fill\hfill{CRA, 2020-2022}
	\item {\href{https://simons.berkeley.edu/programs/fellows}{Simons-Berkeley Research Fellowship}} \hspace*\fill\hfill{Simons Institute, 2019}
	\item { The National Initiative for Modeling and Simulation Research 
		Fellowship  }\\  \hspace*\fill\hfill{UT Austin, 2014-2018}
%	\item {\href{https://cms.caltech.edu/events/90169}{Young Investigators Lecturer award}} \hspace*\fill\hfill{Caltech, 2021}
	\item { \href{https://www.oden.utexas.edu/news-and-events/news/Oden-Institute-Outstanding-Dissertation-Award-Winner-2021/}{Outstanding Dissertation Award}}\hspace*\fill\hfill{Oden Institute, 2021}
%	\item {Dissertation Writing Fellowship} \hspace*\fill\hfill{UT Austin, 2021}
	\item {\href{https://ml.umd.edu/rising-stars}{Rising Star for Machine Learning}} \hspace*\fill\hfill{University of Maryland, 2021}
	\item {\href{https://risingstars21-eecs.mit.edu/lei-qi/}{Rising Star for EECS}} \hspace*\fill\hfill{UIUC, 2019 \& MIT, 2021}
	\item {Rising Star for Computational and Data Science} \hspace*\fill\hfill{UT Austin, 2020}
	%\item {Gold medal ($5^{th}$ place) in China Girls Math Olympiad (CGMO, an international competition with a proof-based format similar to the International Math Olympiad)} \hfill{China, 2009}
%	\item {Meritorious Winner for The Mathematical Contest in Modeling (MCM) }\\
	%\hspace*\fill{COMAP, 2014}
%	\item {The Excellence Scholarship (top honor)} \hfill{Zhejiang Univ, 2014}
	%\item {First Prize for CMC (the Mathematics competition of Chinese College Student)} \hspace*\fill\hfill{China, 2012}
	%\item {First Prize Scholarship \& Merit Student} \hfill{Zhejiang Univ, 2010-2014}
	%\item {Third Prize for ACM Programming Contest} \hfill{Zhejiang Univ, 2012}
	%\item {First Prize for National Olympiad in Informatics in Provinces (NOIP)}\\
	%\hspace*\fill \hfill{China, 2007(perfect score), 2008}
\end{itemize}






%\section{SOFTWARE}

%----------------------------------------------------------------------------------------
%	INDUSTRY EXPERIENCE
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%	Technology SKILLS SECTION
%----------------------------------------------------------------------------------------

\section{Thesis} 
\textbf{Qi Lei}, ``Provably effective algorithms for min-max optimization" \hfill{May 2020} \\ 
\textit{\href{https://www.oden.utexas.edu/news-and-events/news/Oden-Institute-Outstanding-Dissertation-Award-Winner-2021/}{Received the 2021 Outstanding Dissertation Award}, Oden Institute}

%\newpage
\section{Publications\\
	\vspace{4pt}
{\footnotesize ($*$ indicates\\ $\alpha$-$\beta$ order) }}
\begin{enumerate}
\item{Sheng Liu, Zihan Wang, \textbf{Qi Lei}, ``Data Reconstruction Attacks and Defenses: A Systematic Evaluation", \textit{to appear at AISTATS 2025}} 
	
\item{Tao Wen, Zihan Wang, Quan Zhang, \textbf{Qi Lei}, ``Elastic Representation: Mitigating Spurious Correlations for Group Robustness", \textit{to appear at AISTATS 2025} }
	
\item{Ziliang Samuel Zhong, Xiang Pan, \textbf{Qi Lei}, ``Bridging Domains with Approximately Shared Features", \textit{to appear at AISTATS 2025} }
	
\item{Jianwei Li, Yijun Dong, \textbf{Qi Lei}, ``Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining", \textit{to appear at CPAL 2025} }
	
\item{Tao Wen, Elynn Chen, Yuzhou Chen, \textbf{Qi Lei}, ``Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based Framework for Effective Label Propagation", \textit{to appear at CPAL 2025} }
	
\item{Qi Zhang, Yifei Wang, Jingyi Cui, Xiang Pan, \textbf{Qi Lei}, Stefanie Jegelka, Yisen Wang, ``Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness", \textit{to appear at ICLR 2025}}
	
	
	
\item{Yijun Dong, Hoang Phan, Xiang Pan, \textbf{Qi Lei}, ``Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning", \textit{NeurIPS 2024} }
	
\item{Qian Yu, Yining Wang, Baihe Huang, \textbf{Qi Lei}, Jason D Lee, ``Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity", \textit{NeurIPS 2024} }
	
\item{Hoang Phan, Andrew G Wilson, \textbf{Qi Lei}, ``Controllable Prompt Tuning For Balancing Group Distributional Robustness", \textit{International Conference of Machine Learning (ICML), 2024}}

\item{Hong J Jeon, Jason D Lee, \textbf{Qi Lei}, Benjamin Van Roy, ``An Information-Theoretic Analysis of In-Context Learning", \textit{International Conference of Machine Learning (ICML), 2024}
}

\item{Qian Yu, Yining Wang, Baihe Huang, \textbf{Qi Lei}, Jason D Lee, ``Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms", \textit{Advances in Neural Information Processing Systems 36, 2023}}
	
\item{Yijun Dong, Kevin Miller, \textbf{Qi Lei}, Rachel Ward, ``Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering", \textit{Advances in Neural Information Processing Systems 36, 2023}}
	
\item{Jianwei Li, \textbf{Qi Lei}, Wei Cheng, Dongkuan Xu, ``Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models", \textit{EMNLP conference, 2023: 1229-1247}}

\item{Jianwei Li, Weizhi Gao, \textbf{Qi Lei}, Dongkuan Xu, ``Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection", \textit{EMNLP findings, 2023: 11407-11423}}
	
\item{Tianci Liu, Tong Yang, Quan Zhang, \textbf{Qi Lei}, ``Optimization for Amortized Inverse Problems", \textit{International Conference of Machine Learning (ICML), 2023}}
	
\item{Zihan Wang, Jason Lee, \textbf{Qi Lei}, ``Reconstructing Training Data from Model Gradient, Provably", \textit{International Conference on
		Artificial Intelligence and Statistics (AISTATS), 2023: 6595-6612}}
	
\item{Shuo Yang, Yijun Dong, Rachel Ward, Inderjit Dhillon, Sujay Sanghavi, \textbf{Qi Lei}, ``Sample Efficiency of Data Augmentation Consistency Regularization", \textit{International Conference on
			Artificial Intelligence and Statistics (AISTATS), 2023: 3825-3853}}
	
\item{Kurtland Chua, \textbf{Qi Lei}, Jason Lee, ``Provable Hierarchy-Based Meta-Reinforcement Learning", \textit{International Conference on
		Artificial Intelligence and Statistics (AISTATS), 2023: 10918-10967}}
	
	\item{Qian Yu, Yining Wang, Baihe Huang, \textbf{Qi Lei}, Jason Lee, ``Optimal Sample Complexity Bounds for Non-convex Optimization under Kurdyka-Lojasiewicz Condition", \textit{International Conference on
			Artificial Intelligence and Statistics (AISTATS), 2023: 6806-6821}}
	

	
\item{Minhao Cheng, \textbf{Qi Lei}, Pin-Yu Chen, Inderjit Dhillon, Cho-Jui Hsieh, ``Cat: Customized adversarial training for improved robustness", \textit{International Joint Conference on Artificial Intelligence (IJCAI), 2022: 673-679}}
	
		\item {Jason D. Lee*, \textbf{Qi Lei}*, Nikunj Saunshi*, Jiacheng Zhuo*, ``Predicting What You Already Know Helps: Provable Self-Supervised Learning”, \textit{Neural Information Processing Systems (NeurIPS), 2021: 309-323}}


\item {Baihe Huang*, Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*, Runzhe Wang*, Jiaqi Yang*,  ``Optimal Gradient-based Algorithms for Non-concave Bandit Optimization", 	\textit{Neural Information Processing Systems (NeurIPS), 2021: 29101-29115} }


\item 	{ Kurtland Chua, \textbf{Qi Lei}, Jason D. Lee, ``How Fine-Tuning Allows for Effective Meta-Learning”, \textit{Neural Information Processing Systems (NeurIPS), 2021: 8871-8884} }



\item {Baihe Huang*, Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*, Runzhe Wang*, Jiaqi Yang*, ``Going Beyond Linear RL: Sample Efficient Neural Function Approximation", \textit{Neural Information Processing Systems (NeurIPS), 2021: 8968-8983} }
	
	
\item{ \textbf{Qi Lei}, Wei Hu, Jason D. Lee.	
``Near-Optimal Linear Regression under Distribution Shift", \textit{ International Conference of Machine Learning (ICML), 2021: 6164-6174}
}	
\item{Tianle Cai*, Ruiqi Gao*, Jason D Lee*, \textbf{Qi Lei}*.  ``A Theory of Label Propagation for Subpopulation Shift", \textit{International Conference of Machine Learning (ICML), 2021: 1170-1182
} }
\item{Jay Whang, \textbf{Qi Lei}, Alexandros G. Dimakis, ``Solving Inverse Problems with a Flow-based Noise Model", \textit{International Conference of Machine Learning (ICML), 2021: 11146-11157
 }}
	\item{Simon S. Du*, Wei Hu*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*, ``Few-Shot Learning via Learning the Representation, Provably'', \textit{International Conference on Learning Representations (ICLR), 2021} }
	
	\item{\textbf{Qi Lei}*, Sai Ganesh Nagarajan*, Ioannis Panageas*, Xiao Wang*, ``Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes'',\textit{ International Conference on
			Artificial Intelligence and Statistics (AISTATS), 2021: 1441-1449}}
	\item{ Xiao Wang, \textbf{Qi Lei}, Ioannis Panageas, ``Fast Convergence of Langevin Dynamics on Manifold: Geodesics meet Log-Sobolev'', \textit{Neural Information Processing Systems (NeurIPS), 2020 }  }
%\item{Jason D. Lee*, \textbf{Qi Lei}*, Nikunj Saunshi*, Jiacheng Zhuo*, ``Predicting What You Already Know Helps: Provable Self-Supervised Learning'', \textit{NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice} }
%\item{Jay Whang, \textbf{Qi Lei}, Alexandros G. Dimakis, ``Compressed Sensing with Invertible Generative Models and Dependent Noise'', \textit{NeurIPS 2020 Workshop on Deep Learning and Inverse Problems}} 	
\item{Jiacheng Zhuo, \textbf{Qi Lei}, Alexandros G. Dimakis, Constantine 
      Caramanis.\\ ``Communication-Efficient Asynchronous Stochastic 
    Frank-Wolfe over Nuclear-norm Ball'', \textit{ International Conference on Artificial Intelligence and Statistics (AISTATS), 2020: 1464-1474} }
\item{\textbf{Qi Lei}, Jason Lee, Alexandros G. Dimakis, Contantinos 
  Daskalakis. \\ ``SGD Learns One-Layer Networks in WGANs'', 
    \textit{International Conference of Machine Learning (ICML), 2020: 5799-5808}}
  \item{ \textbf{Qi Lei}, Jiacheng Zhuo, Constantine Caramanis, Inderjit S. 
    Dhillon, Alexandros G. Dimakis, ``Primal-Dual Block Generalized Frank-Wolfe'', 
  \textit{Neural Information Processing Systems (NeurIPS), 2019: 13866-13875} }
  \item{ \textbf{Qi Lei}, Ajil Jalal, Inderjit S. Dhillon, Alexandros G. 
      Dimakis, ``Inverting Deep Generative models, One layer at a time'', 
    \textit{Neural Information Processing Systems (NeurIPS), 2019: 13910-13919} }

  \item{ \textbf{Qi Lei}, Jinfeng Yi, Roman Vaculin, Lingfei Wu, Inderjit S. 
      Dhillon, ``Similarity Preserving Representation Learning for Time Series 
      Analysis'', \textit{International Joint Conference on Artificial 
    Intelligence (IJCAI), 2019: 2845-2851}}
  \item{\textbf{Qi Lei}, Lingfei Wu, Pin-Yu Chen, Alexandros G. Dimakis, Inderjit S. 
    Dhillon, Michael Witbrock, ``Discrete Adversarial Attacks and Submodular 
    Optimization with Applications to Text Classification'', \textit{Systems and Machine 
  Learning (MLSys), 2019} (\textbf{covered by \href{https://www.nature.com/articles/d41586-019-01510-1}{Nature News} )  } }
\item{Jinfeng Yi, \textbf{Qi Lei}, Wesley M Gifford, Ji Liu, Junchi Yan, Bowen Zhou, ``Fast Unsupervised Location Category Inference from Highly Inaccurate Mobility Data'', \textit{SIAM International Conference on Data Mining 2019: 55-63}}
\item{Zhewei Yao, Amir Gholami, \textbf{Qi Lei}, Kurt Keutzer, Michael W. 
  Mahoney, ``Hessian-based Analysis of Large Batch Training and Robustness to 
Adversaries'', \textit{Neural Information Processing Systems (NIPS), 2018: 4954-4964}}
  \item{Jiong Zhang, \textbf{Qi Lei}, Inderjit S. Dhillon. 
        ``Stabilizing Gradients for Deep 
       Neural Networks via Efficient SVD Parameterization'', \textit{
    International Conference of Machine Learning (ICML), 2018: 5801-5809}}
\item{Jinfeng Yi, \textbf{Qi Lei}, Junchi Yan, Wei Sun, ``Session expert: A lightweight conference session recommender system'', \textit{IEEE International Conference on Big Data (Big Data), 2018: 1677-1682}}
  \item{Lingfei Wu, Ian En-Hsu Yen, Jinfeng Yi, Fangli Xu, \textbf{Qi Lei}, Michael Witbrock.
    ``Random Warping Series: A Random Features Method for Time-Series Embedding'', \textit{International Conference on Artificial Intelligence and Statistics (AISTATS), 2018: 793-802}}
\item{Hsiang-fu Yu, Cho-Jui Hsieh, \textbf{Qi Lei}, Inderjit S. Dhillon. 
      ``A Greedy Approach for Budgeted Maximum 
      Inner Product Search'', \textit{Neural Information Processing Systems 
      (NIPS), 2017: 5453-5462}}
    \item{\textbf{Qi Lei}, Enxu Yen, Chao-yuan Wu, Inderjit S. Dhillon, Pradeep 
        Ravikumar, ``Doubly Greedy Primal-Dual Coordinate Methods on Sparse Empirical 
  Risk Minimization'', \textit{International Conference of Machine 
    Learning (ICML), 2017: 2034-2042}}
\item{Rashish Tandon, \textbf{Qi Lei}, 
    Alexandros G. Dimakis, Nikos Karampatziakis, ``Gradient Coding: Avoiding 
    Stragglers in Distributed Learning'', \textit{International Conference of 
  Machine Learning (ICML), 2017: 3368-3376}}
  \item {\textbf{Qi Lei},
      Kai Zhong, Inderjit S. Dhillon, ``Coordinate-wise Power Method'', 
  \textit{Neural Information Processing System (NIPS), 2016: 2056-2064}}		

%\item {\textbf{Qi Lei}, Jinfeng Yi, Roman Vaculin, Inderjit Dhillon. "Similarity Preserving Representation Learning for Time Series Analysis", \textit{Submitted for publication}}
%\item {Rashish Tandon, \textbf{Qi Lei}, Alexandros G. Dimakis, Nikos  Karampatziakis. "Gradient Coding", \textit{ML Systems Workshop at NIPS, Dec. 2016}}
	\item {Arnaud Vandaele, Nicolas Gillis, \textbf{Qi Lei},
      Kai Zhong, Inderjit S. Dhillon, ``Coordinate Descent Methods for
		Symmetric Nonnegative Matrix Factorization'', \textit{IEEE Transactions on 
Signal Processing}, 64.21 (2016): 5571-5584}	
%	\item {Hsiang-Fu Yu, Cho-Jui Hsieh, \textbf{Qi Lei}, Inderjit S. Dhillon. "A Greedy Approach for Budgeted Maximum Inner Product Search", \textit{arXiv:1610.03317v1}}

	\item {Maria R. D'Orsogna, \textbf{Qi Lei}, Tom Chou, ``First assembly times and equilibration in stochastic coagulation-fragmentation", \textit{Journal of Chemical Physics}, 2015: 143.1, 014112}
		\item {Jiazhou Chen, \textbf{Qi Lei}, Yongwei Miao, Qunsheng Peng, ``Vectorization of Line Drawing Image based on Junction Analysis", \textit{Science China Information Sciences}, 2014:1-14}
	\item {Jiazhou Chen, \textbf{Qi Lei}, Fan Zhong, Qunsheng Peng, ``Interactive Tensor Field Design Based on Line Singularities", \textit{Proceedings of the 13th International CAD /Graphics}, 2013}
\end{enumerate}



\section{Workshop\\Articles}
\begin{enumerate}
	\item{Yijun Dong, Xiang Pan, Hoang Phan, \textbf{Qi Lei}, ``Randomly Pivoted V-optimal Design: Fast Data Selection under Low Intrinsic Dimension", \textit{ML and Compression-NeurIPS 2024 }}
	\item{Jianwei Li, Sheng Liu, \textbf{Qi Lei}, ``‘Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning", \textit{FL@FM-NeurIPS 2023}
}
	\item{Chun-Yin Huang, \textbf{Qi Lei}, Xiaoxiao Li, ``Efficient Medical Image Assessment via Self-supervised Learning", \textit{DALI@MICCAI Workshop, 2022: 102-111} (\textbf{with Best Paper Honorable Mention})}
%	\item{Tianci Liu, Quan Zhang, \textbf{Qi Lei}, ``PANOM: Automatic Hyper-parameter Tuning for Inverse Problems", \textit{NeurIPS 2021 Workshop on Deep Learning and Inverse Problems} }
	\item {Kaixuan Huang*, Sham M. Kakade*, Jason D. Lee*, \textbf{Qi Lei}*,  ``A Short Note on the Relationship of Information Gain and Eluder Dimension", \textit{ICML 2021 Workshop on Reinforcement Learning Theory} }
%	\item {Jason D. Lee*, \textbf{Qi Lei}*, Nikunj Saunshi*, Jiacheng Zhuo*, ``Predicting What You Already Know Helps: Provable Self-Supervised Learning”, \textit{NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice}}
	
%\item{Jay Whang, \textbf{Qi Lei}, Alex Dimakis, 	``Compressed Sensing with Invertible Generative Models and Dependent Noise", \textit{NeurIPS 2020 Workshop: Deep Learning and Inverse Problems}}

%\item{\textbf{Qi Lei}, Ajil Jalal, Inderjit Dhillon, Alexandros Dimakis, ``Inverting Deep Generative models, One layer at a time", \textit{ICML 2019 Workshop on Invertible Neural Networks and Normalizing Flows} 	}
	
%\item{Rashish Tandon, \textbf{Qi Lei}, Alexandros G. Dimakis, Nikos Karampatziakis,	``Gradient Coding", \textit{NIPS 2016 Workshop on ML Systems (MLSys)} }
	
\end{enumerate} 

%\section{Under\\Review\\Preprints}
%\begin{enumerate}
%\item{Shuo Yang, Yijun Dong, Rachel Ward, Inderjit S Dhillon, sujay sanghavi, \textbf{Qi Lei}, ``Theoretical Analysis of Consistency Regularization with Limited Augmented Data", \textit{under review}}  

%\item{Kurtland Chua, \textbf{Qi Lei}, Jason D Lee, ``Provable Hierarchy-Based Meta-Reinforcement Learning", \textit{arXiv preprint} }

%\item{Lemeng Wu, Mao Ye, \textbf{Qi Lei}, Jason D. Lee, and Qiang Liu, ``Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting”, \textit{arXiv preprint}}

%\item{ Minhao Cheng, \textbf{Qi Lei}, Pin-Yu Chen, Inderjit Dhillon, Cho-Jui Hsieh, ``CAT: Customized Adversarial Training for Improved Robustness”, \textit{arXiv preprint}}

%\end{enumerate} 

\section{Patents}\begin{itemize}
	\item{``Method and System for General and Efficient Time Series Representation 
		Learning via Dynamic Time Warping."\\
		\textbf{Q. Lei}, J. Yi, R. Vaculin, and W. Sun}
	
	\item{``Real-Time Cold Start Recommendation and Rationale within a Dialog System".\\
		\textbf{Q. Lei}, J. Yi, R. Vaculin, M. Pietro}
\end{itemize}
%----------------------------------------------------------------------------------------
%	PROFESSIONAL EXPERIENCE SECTION
%----------------------------------------------------------------------------------------
\begin{comment}
\section{SOFTWARE}
 \textit{Github: }\url{https://github.com/cecilialeiqi/}\\
 {\sl SPIRAL} \hfill May 2016 - July 2017
 \begin{itemize}
   \item Feature representation learning of any time series data
    \end{itemize}
 {\sl DDI} \hfill Jan 2017 - May 2017
 \begin{itemize}
   \item Use an inductive tensor completion based methods to predict drug-drug interactions
   \end{itemize}
   {\sl NUTF} \hfill August 2016 - February 2017
   \begin{itemize}
     \item Negative-Unlabeled Tensor Factorization for Location\ Context 
       Inference from Inaccurate Mobility Data
      \end{itemize}
 \end{itemize} 
\end{comment}

\section{Teaching}


{\sl Center for Data Science, New York University} 
\begin{itemize}
	\item Linear Algebra and Optimization: {\sl Instructor} \hfill Fall 2024
	\item Modern Topics in Statistical Learning Theory:  {\sl Instructor} \hfill Spring 2023,2024
\end{itemize}


{\sl Courant Institute Mathematics Department, New York University} 
\begin{itemize}
	\item Mathematical Statistics: {\sl Instructor} \hfill Fall 2023, 2024
	\item Probability and Statistics: {\sl Instructor} \hfill Fall 2022
\end{itemize}

{\sl Department of Electrical and Computer Engineering, Princeton} 
\begin{itemize}
	\item Theory of Deep Learning: Representation and Weakly Supervised Learning: {\sl Teaching Assistant} \hfill Fall 2020
\end{itemize}


{\sl Department of Electrical and Computer Engineering, UT Austin} 
\begin{itemize}
  \item Scalable Machine Learning: {\sl Teaching Assistant} \hfill Fall 2019
  \end{itemize}

{\sl Oden Institute for Computational Engineering and Sciences, UT Austin} 
\begin{itemize}
  \item Mathematical Methods in Applied Engineering and Sciences: {\sl Instructor Intern} \hfill Fall 2015
\end{itemize}


\section{Industry\\Experiences}

{\sl Facebook/Photo\&Video Search} \hfill June 2018 - September 2018

\begin{itemize}
	\item  Explored offline/online evaluation gaps by estimating expected number of clicks based on historical logging data.
\end{itemize}

{\sl Amazon/A9 Product Search} \hfill May 2017 - August 2017
\begin{itemize}
	\item Inline search suggestions: used deep learning methods for NLP user search tasks.
\end{itemize}

{\sl Amazon Web Services (AWS Deep Learning Team)} \hfill January 2017 - April 2017
\begin{itemize}
	\item Documentations for MXNet: a deep learning framework designed for 
	both efficiency and flexibility.
\end{itemize}

{\sl IBM Thomas J. Watson Research Center} \hfill May 2016 - October 2016
\begin{itemize}
	\item  Clients' propensity prediction of trading options
	Partnered with one of the largest American financial companies on a challenge problem of predicting its clients' propensity of trading options
	\item Create World of Watson Session recommendation system:\\
	\url{https://myibm.ibm.com/events/wow/watson/}
\end{itemize} 



\section{Service}
Co-organizing the \href{https://mad.cds.nyu.edu/}{Math and Data seminar}, New York University, since 2022 

Co-organizing mini-symposium on ``Efficient computation and learning with randomized sampling and pruning", at SIAM MDS 2024

Co-organizing the \href{https://meta-learn.github.io/2022/}{workshop on meta-learning} at NeurIPS 2022

%Co-organizing Mathematical Data Science Reading Group, which is a weekly departmental seminar series on Machine Learning Theory in ECE, Princeton, 2020-2021 

Student mentor,  Oden Institute, 2018-2020
%{\sl 2021:} Serve as a mentor at the mentorship program of WiML-T (Women in Machine Learning Theory)

{\sl Conference Reviewer:} MLSys, COLT, STOC, NeurIPS, ICML, ICLR, AISTATS, AAAI, and more 
%MLSys (19,20,Meta-reviewer'21, TPC'22), COLT (21,22,23), STOC (20), NeurIPS (16,17,18,19,20,21), ICML (18,19,20,21), ICLR (18,19,20,21), AISTATS (18,19,20,21), AAAI (20,21), ACML (19), and more

{\sl Journal Reviewer:} MOR, JMLR, Annuls of Statistics, EJMS, JSAIT, TNNLS, TKDE, ISIT, IT, Computer Speech \& Language, and more %JSAIT(20), MOR (18,19,20), TNNLS (19,20), TKDE (19), ISIT (17,18), TIIS (17), IT (16,17), JMLR, and more


\begin{comment} 

 \section{Invited Talks}
 {``Label Propagation on Self-Supervised Representation Space."}
 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
	\item \href{https://simons.berkeley.edu/workshops/games2022-1}{Adversarial Approaches in Machine Learning Workshop}, Simons Institute, CA, 2022 
	\item \href{http://zke.fas.harvard.edu/HawaiiConference/Main.html}{New Advances in Statistics and Data Science}, Honolulu, Hawaii, 2022
\end{itemize}
 
{``Optimal Gradient-based Algorithms for Non-concave Bandit Optimization."}
 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 		\item \href{http://bliss.eecs.berkeley.edu/Seminar/fa21/qi.html}{BLISS seminar}, UC Berkeley, virtual 2021
 		\item \href{https://simons.berkeley.edu/talks/optimal-gradient-based-algorithms-non-concave-bandit-optimization}{Sampling Algorithms and Geometries on Probability Distributions Workshop}, Simons Institute, CA, 2021 
 	\end{itemize}
 


{``From Reconstruction to Similarity-based Self-supervised Learning."}
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
	\item \href{https://math.dartmouth.edu/calendar/more.php?event_id=2849}{Dartmouth Applied \& Computational Mathematics Seminar}, virtual, 2022 
\end{itemize}
{``Predicting What You Already Know Helps: Provable Self-Supervised Learning.''
	 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
	 		\item Neural Information Processing Systems, virtual, 2021
			\item \href{https://www.ifml.institute/events}{Institute for Foundations of Machine Learning}, virtual, 2020
			\item \href{https://www.oneworldml.org/past-events}{One-World ML seminar}, virtual, 2020
	 		\item UW-Madison, virtual, 2020	 
	 	\end{itemize}
}


{``Provable representation learning.''
\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
	\item 		\href{https://sites.google.com/view/slowdnn/}{Young Researcher Spotlight Talk} at ``Seeking Low-dimensionality in Deep Learning'' workshop, virtual, 2020 
	\item  Microsoft Research (Redmond and NY), virtual, 2021  
	\item \href{https://cms.caltech.edu/events/90169}{Caltech Young Investigators Lecture Series}, virtual, 2021
	\item \href{https://twitter.com/ml_umd/status/1458125253197058054}{Rising star presentation} at the University of Maryland, virtual, 2021
	\item  \href{https://talks-calendar.app.ist.ac.at/events/3329}{ELLIS Talk at IST Austria}, virtual, 2021
	\item \href{https://www.ece.gatech.edu/calendar/day/2021/12/03/108208?utm_source=ECE+Lists+2019&utm_campaign=a5a1732812-EMAIL_CAMPAIGN_2019_08_06_08_30_COPY_01&utm_medium=email&utm_term=0_0324b14402-a5a1732812-103287103}{CSIP seminar at Georgia Tech}, virtual, 2021
	\item AlgML seminar, Princeton University, NJ, 2022
\end{itemize}
	 }
 
 {`` Few-Shot Learning via Learning the Representation, Provably.''
 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 		\item \href{https://iclr.cc/media/Slides/iclr/2021/virtual(07-00-00)-07-00-00UTC-2535-few-shot_learni.pdf}{International Conference on Learning Representations}, virtual, 2021
 		\item 	IAS, Princeton, NJ, 2020 
 		\item  Simons Institute Reunion, virtual, 2020 
 		\item UC Berkeley, virtual, 2020	
 	\end{itemize}
 }


{``SGD Learns One-Layer Networks in WGANs.''
  	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
  		\item \href{https://papertalk.org/papertalks/6195}{International Conference of Machine Learning} (ICML), virtual, 2020
  		\item \href{https://simons.berkeley.edu/talks/sgd-learns-one-layer-networks-wgans}{Workshop on Learning and Testing in High Dimensions}, Simons Institute, 2020
  	\end{itemize} 
  }
 % \newpage 
{``Deep Generative models and Inverse Problems.''
	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
		\item        \href{http://faculty.smu.edu/sxu/SIAMTXLA19/submissions.html}{Minisymposium on Machine Learning for Solving Partial Differential 
		Equations and Inverse Problems}, 2019 SIAM Texas-Louisiana Section, Dallas,
		TX, 2019 
		\item Princeton, virtual, 2020 
		\item  Google Research, virtual, 2021
	\end{itemize}
}


{``Similarity Preserving Representation Learning for Time Series Analysis.''
	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
		\item \href{https://www.ijcai.org/proceedings/2019/394}{The 28th International Joint Conference on Artificial Intelligence} (IJCAI), Macao, China, 2019
	\end{itemize}
}
{``Discrete Adversarial Attacks 
 and Submodular Optimization with Applications to Text Classification.''
 \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 	\item  Simons-Berkeley Fellows Talk, Berkeley, CA, 2019 
 	\item \href{https://www.youtube.com/watch?v=UnakqzVLVLI}{The Conference on Systems and Machine Learning}  (SysML), Stanford, CA, 
 2019
 \end{itemize} 
}
{``Recent 
     Advances in Primal-Dual Coordinate Methods for ERM.''
     \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
     	\item     \href{https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=66077}{Minisymposium on Recent Progress in Coordinate-wise Descent Methods}, SIAM Conference on Computational Science and Engineering, Spokane, WA, 2019
     	\item  International Conference of Machine Learning (ICML), Sydney, 2017  
     \end{itemize}
}
{``Coordinate Descent Methods for Matrix Factorization.''
 	\begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
 		\item   \href{https://archive.siam.org/meetings/an16/}{ Minisymposium on Recent Advances in Nonnegative Matrix Factorization}, SIAM 
 		Annual Meeting, Boston, MA, 2016
 	\end{itemize}
}


% \end{item}
\begin{comment} 
\section{Programming \\ Skills} 
\begin{itemize}
	\item[]  C/C++(proficient), Python(proficient), Matlab(proficient), C\#(prior experience)
  \item[]  Familiar with Deep Learning packages(Pytorch, Tensorflow, Theano, MXNet)
\end{itemize}
% consider add software?
\end{comment} 
\end{resume}
\end{document}
