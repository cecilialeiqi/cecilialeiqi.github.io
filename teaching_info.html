<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>DA-GA3001: Modern Topics in Statistical Learning Theory</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">DS-GA3001</div>
<div class="menu-item"><a href="teaching.html">Spring&nbsp;2024</a></div>
<div class="menu-item"><a href="teaching2023.html">Spring&nbsp;2023</a></div>
<div class="menu-item"><a href="teaching_info.html" class="current">Course&nbsp;info</a></div>
<div class="menu-item"><a href="index.html">Homepage</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>DA-GA3001: Modern Topics in Statistical Learning Theory</h1>
</div>
<div class="infoblock">
<div class="blockcontent">
<p>Center for Data Science, New York University, Spring 2024</p>
<p>Instructor: Qi Lei</p>
</div></div>
<p>Welcome to the DA-GA3001 course webpage for Spring 2024.</p>
<h2>Course Description:</h2>
<p>This course is a graduate-level topic course focusing on the theoretical grounding and statistical properties
of the modern learning algorithms â€” with a focus on feature learning and AI safety.
The intended topics to cover include: basics in machine learning, optimization and generalization bound,
followed by the introduction and theoretical understanding surrounding meta-learning, self-supervised learning,
domain adaptation, and AI safety.</p>
<p>To benefit from this class, strong linear algebra, probability, and optimization background are required.
Students should be familiar with basic machine learning and deep learning concepts.
The class consists of 3 units. In the first unit, we will cover the more standard theoretical analysis
tools used in deep learning including stochastic gradient, uniform convergence theory and statistical learning
theory.</p>
<p>After the first unit, the course will move on to specific topics (Unit 2: feature learning and Unit 3: AI
safety). Since some topics covered in this course are quite recent, some content will be based on recent
papers instead of a textbook.</p>
<h2>Resources for the class: </h2>
<p>Even though the content of this course is not based on a specific textbook, the following materials are good references for certain topics of the course.</p>
<ul>
<li><p><a href="http://web.stanford.edu/class/stats214/">Stanford CS 229M notes</a></p>
</li>
</ul>
<ul>
<li><p><a href="http://www.cs.umd.edu/class/fall2022/cmsc828W/info.html">CMSC 828W notes: Foundations of Deep Learning</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E">Wainwright Book (High dimensional statistics non-asymptotic</a></p>
</li>
</ul>
<ul>
<li><p><a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf">Vershynin's book</a></p>
</li>
</ul>
<ul>
<li><p><a href="http://www.gautamkamath.com/CS860-fa2020.html">Kamath's CS 860</a></p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2024-05-23 10:33:57 , by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
